# 🐑 Fine-Tuned LLaMA SaaS API

Private LLM inference with fine-tuned LLaMA models.

## 🧰 Stack
- LLaMA
- FastAPI
- Stripe
- Redis

## 🧠 What I’m Learning
- Quantization + fine-tuning
- Stripe metered billing
- Redis caching & queues

## ✅ Roadmap
- [ ] Fine-tune small LLaMA model
- [ ] Deploy with FastAPI
- [ ] Add Redis rate limit/cache
- [ ] Stripe usage-based billing
- [ ] Admin dashboard (optional)

## 🕓 Progress Log
- TBD
